{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import ElementClickInterceptedException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "FED_BASE_URL = \"https://www.federalreserve.gov\"\n",
    "FED_SPEECHES_URL = \"https://www.federalreserve.gov/newsevents/speeches.htm\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating Chrome web driver\n"
     ]
    }
   ],
   "source": [
    "print(\"Instantiating Chrome web driver\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_fed_speeches(start_date, end_date):\n",
    "    START_DATE_XPATH = \"//*[@id=\\\"content\\\"]/div[2]/div/div[1]/form/div[2]/div/div[1]/input\"\n",
    "    END_DATE_XPATH = \"//*[@id=\\\"content\\\"]/div[2]/div/div[1]/form/div[2]/div/div[2]/input\"\n",
    "    SUBMIT_BUTTON_XPATH = \"//*[@id=\\\"content\\\"]/div[2]/div/div[1]/form/div[5]/a\"\n",
    "    driver.get(FED_SPEECHES_URL)\n",
    "    start_date_input_box = driver.find_element(\"xpath\", START_DATE_XPATH)\n",
    "    end_date_input_box = driver.find_element(\"xpath\", END_DATE_XPATH)\n",
    "    start_date_input_box.clear()\n",
    "    start_date_input_box.send_keys(start_date)\n",
    "    end_date_input_box.clear()\n",
    "    end_date_input_box.send_keys(end_date)\n",
    "    submit_button = driver.find_element(\"xpath\", SUBMIT_BUTTON_XPATH)\n",
    "    submit_button.click()\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_next_page():\n",
    "    NEXT_BUTTON_TEXT = \"Next\"\n",
    "    try:\n",
    "        element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.LINK_TEXT, NEXT_BUTTON_TEXT))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "\n",
    "    next_button = driver.find_element(\"link text\", NEXT_BUTTON_TEXT)\n",
    "\n",
    "    if next_button.get_attribute(\"disabled\") is None:\n",
    "        next_button.click()\n",
    "        return True\n",
    "\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_fed_speech_links(all_speeches):\n",
    "    soup = BeautifulSoup(all_speeches, 'html.parser')\n",
    "    fed_page_links = soup.find_all(\"a\", href=True)\n",
    "    fed_speech_transcript_links = [link['href'] for \\\n",
    "            link in fed_page_links if \"/speech/\" in link['href']]\n",
    "    return fed_speech_transcript_links"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_fed_speech_transcript_links(fed_speech_links):\n",
    "    DATE_LENGTH = 8\n",
    "    fed_speech_transcript_links = []\n",
    "    for link in fed_speech_links:\n",
    "        date = re.findall('[0-9]+', link)\n",
    "        if len(date) != 0:\n",
    "            date = date[0]\n",
    "        if len(date) == DATE_LENGTH:\n",
    "            fed_speech_transcript_links.append(link)\n",
    "    return fed_speech_transcript_links"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def filter_duplicate_links(fed_speech_links):\n",
    "    uniq_links = set(fed_speech_links)\n",
    "    return list(uniq_links)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_fed_speeches_body(fed_speech_links):\n",
    "    all_fed_speeches = []\n",
    "    for link in fed_speech_links:\n",
    "        all_fed_speeches.append(requests.get(FED_BASE_URL + link).text)\n",
    "    return all_fed_speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_speech(fed_speech_links, fed_speeches_source):\n",
    "    fed_speech_content = {}\n",
    "    num_speeches = len(fed_speech_links)\n",
    "    for speech_idx in range(num_speeches):\n",
    "        date = re.findall('[0-9]+', fed_speech_links[speech_idx])[0]\n",
    "        soup = BeautifulSoup(fed_speeches_source[speech_idx], \"html.parser\")\n",
    "        speech_text = \"\"\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            speech_text += paragraph.get_text()\n",
    "        speech_link = fed_speech_links[speech_idx].replace(\"/newsevents/speech/\", \"\").replace(\".htm\", \"\")\n",
    "        fed_speech_content[speech_link] = (date, speech_text)\n",
    "    return fed_speech_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def tokenize_corpus(filing_corpus):\n",
    "    filing_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return filing_tokenizer.tokenize(filing_corpus)\n",
    "\n",
    "# a function to remove the stop words from corpus\n",
    "def filter_out_stopwords(tokenized_corpus):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filing_corpus_filtered = [word for word in tokenized_corpus if not \\\n",
    "        word.lower() in stop_words]\n",
    "    return filing_corpus_filtered\n",
    "\n",
    "\n",
    "# a function that filters out numbers from corpus\n",
    "def filter_out_numbers(tokenized_corpus):\n",
    "    return [token for token in tokenized_corpus if not (token.isdigit()\n",
    "                                         or token[0] == '-' and token[1:].isdigit())]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def tokenize_fed_speeches(fed_speeches_dict):\n",
    "    tokenized_speeches = {}\n",
    "    DATE_IDX = 0\n",
    "    CONTENT_IDX = 1\n",
    "    for link, speech in fed_speeches_dict.items():\n",
    "        tokenized_corpus = tokenize_corpus(speech[CONTENT_IDX])\n",
    "        tokenized_corpus = filter_out_stopwords(tokenized_corpus)\n",
    "        tokenized_corpus = filter_out_numbers(tokenized_corpus)\n",
    "        speech_link = link.replace(\"/newsevents/speech/\", \"\").replace(\".htm\",\n",
    "                                                                      \"\")\n",
    "        tokenized_speeches[speech_link] = (speech[DATE_IDX], tokenized_corpus)\n",
    "    return tokenized_speeches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "SPEECHES_DIR = \"fed_speeches/\"\n",
    "\n",
    "def write_to_disc(file_names, file_content):\n",
    "    for file_idx in range(len(file_names)):\n",
    "        file_name = file_names[file_idx].replace('/', '_')\n",
    "        file_name = file_name.replace(\".htm\", \"\").replace(\"_newsevents_speech_\", \"\")\n",
    "        with open(SPEECHES_DIR + file_name + \".txt\", 'w') as f:\n",
    "            f.write(file_content[file_idx])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "SERIALIZATION_DIR = \"serialized_data/\"\n",
    "\n",
    "def serialize_speeches(speeches_dict):\n",
    "    with open(SERIALIZATION_DIR + \"speeches_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(speeches_dict, f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def load_speeches(picked_file_name):\n",
    "    with open(SERIALIZATION_DIR + picked_file_name, \"rb\") as f:\n",
    "        dict = pickle.load(f)\n",
    "    return dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving all links from the fed speeches web page \n",
      "Processed page number:  1\n",
      "Processed page number:  2\n",
      "Processed page number:  3\n",
      "Processed page number:  4\n",
      "Processed page number:  5\n",
      "Processed page number:  6\n",
      "Processed page number:  7\n",
      "Processed page number:  8\n",
      "Processed page number:  9\n",
      "Processed page number:  10\n",
      "Processed page number:  11\n",
      "Processed page number:  12\n",
      "Processed page number:  13\n",
      "Processed page number:  14\n",
      "Processed page number:  15\n",
      "Processed page number:  16\n",
      "Processed page number:  17\n",
      "Processed page number:  18\n",
      "Processed page number:  19\n",
      "Processed page number:  20\n",
      "Processed page number:  21\n",
      "Processed page number:  22\n",
      "Processed page number:  23\n",
      "Processed page number:  24\n",
      "Processed page number:  25\n",
      "Processed page number:  26\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieving all links from the fed speeches web page \")\n",
    "\n",
    "# START_DATE = \"06/01/2022\"\n",
    "# END_DATE = \"10/28/2022\"\n",
    "START_DATE = \"10/29/2012\"\n",
    "END_DATE = \"10/29/2022\"\n",
    "\n",
    "count = 1\n",
    "fed_page_links = []\n",
    "get_fed_speeches(START_DATE, END_DATE)\n",
    "fed_page_links.extend(get_fed_speech_links(driver.page_source))\n",
    "while get_next_page():\n",
    "    fed_page_links.extend(get_fed_speech_links(driver.page_source))\n",
    "    print(\"Processed page number: \", count)\n",
    "    count += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving all speech transcripts from the fed web page \n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieving all speech transcripts from the fed web page \")\n",
    "fed_page_transcript_links = get_fed_speech_transcript_links(fed_page_links)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out duplicate links \n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering out duplicate links \")\n",
    "speech_links = filter_duplicate_links(fed_page_transcript_links)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the body of all fed speeches\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting the body of all fed speeches\")\n",
    "fed_speeches_body = get_fed_speeches_body(speech_links)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing the speech body for each speech\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsing the speech body for each speech\")\n",
    "speeches_text = get_speech(speech_links, fed_speeches_body)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing crawled speeches to disk\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing crawled speeches to disk\")\n",
    "speeches_body_list = [speech[1] for speech in list(speeches_text.values())]\n",
    "write_to_disc(speech_links, speeches_body_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the body of each speech\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizing the body of each speech\")\n",
    "tokenized_speeches = tokenize_fed_speeches(speeches_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing tokenized speeches\n"
     ]
    }
   ],
   "source": [
    "print(\"Serializing tokenized speeches\")\n",
    "\n",
    "serialize_speeches(tokenized_speeches)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of speeches processed is  536\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of speeches processed is \", len(tokenized_speeches))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
